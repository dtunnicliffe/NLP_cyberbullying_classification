{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will be scraping some posts from Twitter in order to test my highest-functioning model on new, real data. Once this is tested and confirmed, I can confidently deploy my cyberbullying detection app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries and packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, recall_score, f1_score, confusion_matrix, precision_recall_curve, plot_confusion_matrix, auc\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular expression to identify non-ascii characters in content\n",
    "non_ascii_regex = r'[^\\x00-\\x7F]+'\n",
    "\n",
    "# defining stop words\n",
    "stopword_list = stopwords.words('english')\n",
    "stopword_list += list(string.punctuation)\n",
    "\n",
    "# function to remove special characters, tokenize, and stem content\n",
    "def process(content):\n",
    "    \n",
    "    # using library re to replace non ascii characters by a space\n",
    "    text = re.sub(non_ascii_regex, ' ', content)\n",
    "\n",
    "    # instantiating TweetTokenizer\n",
    "    tk = TweetTokenizer(strip_handles=True)\n",
    "    # tokenizing the content & removing usernames\n",
    "    tokens = tk.tokenize(content)\n",
    "    \n",
    "    # instantiating stemmer\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    # stemming the tokens and removing the stopwords\n",
    "    clean_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stopword_list:\n",
    "            try:\n",
    "                clean_tokens.append(ps.stem(token.lower()))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "    # return the tokens\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading model\n",
    "with open('pickles/lr_pipeline.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "model.predict(['i hate you'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "\n",
    "    # lemmatize\n",
    "    def lemmadata(doc):\n",
    "        pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "        raw_tokens = nltk.regexp_tokenize(doc, pattern)\n",
    "        tokens = [i.lower() for i in raw_tokens]\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        listed = [w for w in tokens if not w in stop_words]\n",
    "        lemmatized = [wordnet_lemmatizer.lemmatize(word, pos=\"v\") for word in listed]\n",
    "        lemmatized = list(filter(lambda w: w != 'lb', lemmatized))\n",
    "        words = list(filter(lambda w: w in english, lemmatized))\n",
    "        return \" \".join(words)\n",
    "\n",
    "    lemmatized = [lemmadata(post) for post in data]\n",
    "\n",
    "    # picked tfidf vectorizer\n",
    "    tfidf = pickle.load(open(\"pickles/tfidf.pkl\", \"rb\"))\n",
    "\n",
    "    transformed = tfidf.transform(lemmatized)\n",
    "    tfidf_df = pd.DataFrame(transformed.toarray(), columns=tfidf.get_feature_names())\n",
    "\n",
    "    # pickled the list of relevant words\n",
    "    relevant = pickle.load(open(\"pickles/relevantwords.pkl\", \"rb\"))\n",
    "\n",
    "    testset = [tfidf_df[word] for word in relevant if word in tfidf_df.columns]\n",
    "\n",
    "    return pd.DataFrame(testset).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text):\n",
    "    # the model\n",
    "    mnb = pickle.load(open(\"pickles/lr_pipeline.pkl\", \"rb\"))\n",
    "    listtext = [text]\n",
    "    processed = preprocess(listtext)\n",
    "    result = mnb.predict(processed)[0]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
